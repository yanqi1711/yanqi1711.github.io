<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><!-- hexo injector head_begin start --><script>
                (() => {
                    const sw = navigator.serviceWorker
                    const error = () => {console.error('注册 sw 时发生错误，很可能是由于您的浏览器不支持 sw')}
                    if (!sw?.register('/sw.js')?.then(() => {
                        if (!sw.controller) location.reload()
                    })?.catch(error)) error()
                })()
            </script><!-- hexo injector head_begin end --><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>神经网络基础知识 | 雪天困不住的大孩子</title><meta name="author" content="yanqi"><meta name="copyright" content="yanqi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="神经元的数学模型神经网络由基本的神经元组成，图NeuranCell 就是一个神经元的数学&#x2F;计算模型。  图NeuranCell 神经元计算模型  输入 input$(x_1,x_2,x_3)$ 是外界输入信号，一般是一个训练数据样本的多个属性。 权重 weights$(w_1,w_2,w_3)$ 是每个输入信号的权重值，以上面的 $(x_1,x_2,x_3)$ 的例子来说，$x_1$ 的权重可能是">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络基础知识">
<meta property="og:url" content="https://sonna.cn/2021/06/07/NNs/index.html">
<meta property="og:site_name" content="雪天困不住的大孩子">
<meta property="og:description" content="神经元的数学模型神经网络由基本的神经元组成，图NeuranCell 就是一个神经元的数学&#x2F;计算模型。  图NeuranCell 神经元计算模型  输入 input$(x_1,x_2,x_3)$ 是外界输入信号，一般是一个训练数据样本的多个属性。 权重 weights$(w_1,w_2,w_3)$ 是每个输入信号的权重值，以上面的 $(x_1,x_2,x_3)$ 的例子来说，$x_1$ 的权重可能是">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://npm.elemecdn.com/yanqi1711-picx/20220423/028.1qvsvm8ksri8.webp">
<meta property="article:published_time" content="2021-06-07T08:57:16.000Z">
<meta property="article:modified_time" content="2023-09-12T03:45:55.185Z">
<meta property="article:author" content="yanqi">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://npm.elemecdn.com/yanqi1711-picx/20220423/028.1qvsvm8ksri8.webp"><link rel="shortcut icon" href="https://npm.elemecdn.com/yanqi1711-picx/img/kun.webp"><link rel="canonical" href="https://sonna.cn/2021/06/07/NNs/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://npm.elemecdn.com/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/@fancyapps/ui/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: {"appId":"0ZPAR0UJ1S","apiKey":"10c27644e12d4d4d8597754fb959371e","indexName":"hexo","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://npm.elemecdn.com/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://npm.elemecdn.com/flickr-justified-gallery/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '神经网络基础知识',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2023-09-12 03:45:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><!-- hexo injector body_begin start --><script src="/sw-dom.js"></script><!-- hexo injector body_begin end --><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><div class="loading-img"></div><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><script>window.paceOptions = {
  restartOnPushState: false
}

document.addEventListener('pjax:send', () => {
  Pace.restart()
})
</script><link rel="stylesheet" href="/css/progress_bar.css"/><script src="https://npm.elemecdn.com/pace-js/pace.min.js"></script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx@1.0.11/img/me.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="雪天困不住的大孩子"><span class="site-name">雪天困不住的大孩子</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">神经网络基础知识</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-06-07T08:57:16.000Z" title="发表于 2021-06-07 08:57:16">2021-06-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-09-12T03:45:55.185Z" title="更新于 2023-09-12 03:45:55">2023-09-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%9F%A5%E8%AF%86/">知识</a></span></div><div class="meta-secondline"></div></div></div><article class="post-content" id="article-container"><h2 id="神经元的数学模型"><a href="#神经元的数学模型" class="headerlink" title="神经元的数学模型"></a>神经元的数学模型</h2><p>神经网络由基本的神经元组成，图NeuranCell 就是一个神经元的数学/计算模型。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/NeuranCell.1nt86bgitcn4.webp" alt="NeuranCell"></p>
<center>图NeuranCell 神经元计算模型</center>

<h3 id="输入-input"><a href="#输入-input" class="headerlink" title="输入 input"></a>输入 input</h3><p>$(x_1,x_2,x_3)$ 是外界输入信号，一般是一个训练数据样本的多个属性。</p>
<h3 id="权重-weights"><a href="#权重-weights" class="headerlink" title="权重 weights"></a>权重 weights</h3><p>$(w_1,w_2,w_3)$ 是每个输入信号的权重值，以上面的 $(x_1,x_2,x_3)$ 的例子来说，$x_1$ 的权重可能是 $0.92$，$x_2$ 的权重可能是 $0.2$，$x_3$ 的权重可能是 $0.03$。当然权重值相加之后可以不是 $1$。</p>
<h3 id="偏移-bias"><a href="#偏移-bias" class="headerlink" title="偏移 bias"></a>偏移 bias</h3><p>还有个 $b$ 是怎么来的？一般的书或者博客上会告诉你那是因为 $y=wx+b$，$b$ 是偏移值，使得直线能够沿 $Y$ 轴上下移动。这是用结果来解释原因，并非 $b$ 存在的真实原因。从生物学上解释，在脑神经细胞中，一定是输入信号的电平/电流大于某个临界值时，神经元细胞才会处于兴奋状态，这个 $b$ 实际就是那个临界值。亦即当：</p>
<p>$$w_1 \cdot x_1 + w_2 \cdot x_2 + w_3 \cdot x_3 \geq t$$</p>
<p>时，该神经元细胞才会兴奋。我们把t挪到等式左侧来，变成$(-t)$，然后把它写成 $b$，变成了：</p>
<p>$$w_1 \cdot x_1 + w_2 \cdot x_2 + w_3 \cdot x_3 + b \geq 0$$</p>
<p>于是 $b$ 诞生了！</p>
<h3 id="求和计算-sum"><a href="#求和计算-sum" class="headerlink" title="求和计算 sum"></a>求和计算 sum</h3><p>$$<br>\begin{aligned}<br>Z &amp;= w_1 \cdot x_1 + w_2 \cdot x_2 + w_3 \cdot x_3 + b \\<br>&amp;= \sum_{i=1}^m(w_i \cdot x_i) + b<br>\end{aligned}<br>$$</p>
<p>在上面的例子中 $m=3$。我们把$w_i \cdot x_i$变成矩阵运算的话，就变成了：</p>
<p>$$Z = W \cdot X + b$$</p>
<h3 id="激活函数-activation"><a href="#激活函数-activation" class="headerlink" title="激活函数 activation"></a>激活函数 activation</h3><p>求和之后，神经细胞已经处于兴奋状态了，已经决定要向下一个神经元传递信号了，但是要传递多强烈的信号，要由激活函数来确定：</p>
<p>$$A=\sigma{(Z)}$$</p>
<p>如果激活函数是一个阶跃信号的话，会像继电器开合一样咔咔的开启和闭合，在生物体中是不可能有这种装置的，而是一个渐渐变化的过程。所以一般激活函数都是有一个渐变的过程，也就是说是个曲线，如图activation 所示。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/activation.28f5irl7hyf4.webp" alt="activation"></p>
<center>图activation 激活函数图像</center>

<h2 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/TrainFlow.kgq27thztdc.webp" alt="TrainFlow"></p>
<center>图TrainFlow 神经网络训练流程图</center>

<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><blockquote>
<p>反向传播（英语：Backpropagation，缩写为BP）是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。</p>
</blockquote>
<h3 id="正向计算的实例"><a href="#正向计算的实例" class="headerlink" title="正向计算的实例"></a>正向计算的实例</h3><p>假设有一个函数：</p>
<p>$$z = x \cdot y \tag{1}$$</p>
<p>其中:</p>
<p>$$x = 2w + 3b \tag{2}$$</p>
<p>$$y = 2b + 1 \tag{3}$$</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/flow1.34a5s2t4uvm0.webp" alt="flow1"></p>
<center>图flow1 简单线性计算的计算图</center>

<blockquote>
<p>当 $w = 3, b = 4$ 时，会得到图flow2 的结果。</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/flow2.6j6xp0u93jw0.webp" alt="flow2"></p>
<center>图flow2 计算结果</center>

<blockquote>
<p>最终的 $z$ 值，受到了前面很多因素的影响：变量 $w$，变量 $b$，计算式 $x$，计算式 $y$。</p>
</blockquote>
<h3 id="反向传播求解-w"><a href="#反向传播求解-w" class="headerlink" title="反向传播求解 $w$"></a>反向传播求解 $w$</h3><h4 id="求-w-的偏导"><a href="#求-w-的偏导" class="headerlink" title="求 $w$ 的偏导"></a>求 $w$ 的偏导</h4><p>假设目标 $z=150$，先只考虑改变 $w$ 的值，而令 $b$ 值固定为 $4$。</p>
<p>因为 $$z = x \cdot y$$，其中 $$x = 2w + 3b, y = 2b + 1$$</p>
<p>所以：</p>
<p>$$\frac{\partial{z}}{\partial{w}}=\frac{\partial{z}}{\partial{x}} \cdot \frac{\partial{x}}{\partial{w}}=y \cdot 2=18 \tag{4}$$</p>
<p>其中：</p>
<p>$$\frac{\partial{z}}{\partial{x}}=\frac{\partial{}}{\partial{x}}(x \cdot y)=y=9$$</p>
<p>$$\frac{\partial{x}}{\partial{w}}=\frac{\partial{}}{\partial{w}}(2w+3b)=2$$</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/flow3.3p6fbia3bz80.webp" alt="flow3"></p>
<center>图flow3</center>

<h4 id="求-w-的具体变化值"><a href="#求-w-的具体变化值" class="headerlink" title="求 $w$ 的具体变化值"></a>求 $w$ 的具体变化值</h4><p>公式4的含义是：当 $w$ 变化一点点时，$z$ 会产生 $w$ 的变化值18倍的变化。记住我们的目标是让 $z=150$，目前在初始状态时是 $z=162$，所以，问题转化为：当需要 $z$ 从 $162$ 变到 $150$ 时，$w$ 需要变化多少？</p>
<p>既然：</p>
<p>$$<br>\Delta z = 18 \cdot \Delta w<br>$$</p>
<p>则：</p>
<p>$$<br>\Delta w = {\Delta z \over 18}=\frac{162-150}{18}= 0.6667<br>$$</p>
<p>所以：</p>
<p>$$w = w - 0.6667=2.3333$$<br>$$x=2w+3b=16.6667$$<br>$$z=x \cdot y=16.6667 \times 9=150.0003$$</p>
<p>我们一下子就成功地让 $z$ 值变成了 $150.0003$，与 $150$ 的目标非常地接近，这就是偏导数的威力所在。</p>
<h3 id="反向传播求解-b"><a href="#反向传播求解-b" class="headerlink" title="反向传播求解 $b$"></a>反向传播求解 $b$</h3><h4 id="求-b-的偏导"><a href="#求-b-的偏导" class="headerlink" title="求 $b$ 的偏导"></a>求 $b$ 的偏导</h4><p>目标还是让 $z=150$，这次我们令 $w$ 的值固定为 $3$，变化 $b$ 的值。</p>
<p>从复合导数公式来看，这两者应该是相加的关系，所以有：</p>
<p>$$\frac{\partial{z}}{\partial{b}}=\frac{\partial{z}}{\partial{x}} \cdot \frac{\partial{x}}{\partial{b}}+\frac{\partial{z}}{\partial{y}}\cdot\frac{\partial{y}}{\partial{b}}=y \cdot 3+x \cdot 2=63 \tag{5}$$</p>
<p>其中：</p>
<p>$$\frac{\partial{z}}{\partial{x}}=\frac{\partial{}}{\partial{x}}(x \cdot y)=y=9$$<br>$$\frac{\partial{z}}{\partial{y}}=\frac{\partial{}}{\partial{y}}(x \cdot y)=x=18$$<br>$$\frac{\partial{x}}{\partial{b}}=\frac{\partial{}}{\partial{b}}(2w+3b)=3$$<br>$$\frac{\partial{y}}{\partial{b}}=\frac{\partial{}}{\partial{b}}(2b+1)=2$$</p>
<h4 id="求-b-的具体变化值"><a href="#求-b-的具体变化值" class="headerlink" title="求 $b$ 的具体变化值"></a>求 $b$ 的具体变化值</h4><p>公式5的含义是：当 $b$ 变化一点点时，$z$ 会发生 $b$ 的变化值 $63$ 倍的变化。记住我们的目标是让 $z=150$，目前在初始状态时是 $162$，所以，问题转化为：当我们需要 $z$ 从 $162$ 变到 $150$ 时，$b$ 需要变化多少？</p>
<p>既然：</p>
<p>$$\Delta z = 63 \cdot \Delta b$$</p>
<p>则：</p>
<p>$$<br>\Delta b = \frac{\Delta z}{63}=\frac{162-150}{63}=0.1905<br>$$</p>
<p>所以：<br>$$<br>b=b-0.1905=3.8095<br>$$<br>$$x=2w+3b=17.4285$$<br>$$y=2b+1=8.619$$<br>$$z=x \cdot y=17.4285 \times 8.619=150.2162$$</p>
<p>这个结果也是与 $150$ 很接近了，但是精度还不够。再迭代几次，直到误差不大于 <code>1e-4</code> 时，我们就可以结束迭代了，对于计算机来说，这些运算的执行速度很快。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/b.73xe3i1wxoo0.webp" alt="b" style="zoom:25%;" />

<h3 id="同时求解-w-和-b-的变化值"><a href="#同时求解-w-和-b-的变化值" class="headerlink" title="同时求解 $w$ 和 $b$ 的变化值"></a>同时求解 $w$ 和 $b$ 的变化值</h3><p>这次我们要同时改变 $w$ 和 $b$，到达最终结果为 $z=150$ 的目的。</p>
<p>已知 $\Delta z=12$，我们不妨把这个误差的一半算在 $w$ 的账上，另外一半算在 $b$ 的账上：</p>
<p>$$\Delta b=\frac{\Delta z / 2}{63} = \frac{12/2}{63}=0.095$$</p>
<p>$$\Delta w=\frac{\Delta z / 2}{18} = \frac{12/2}{18}=0.333$$</p>
<ul>
<li>$w = w-\Delta w=3-0.333=2.667$</li>
<li>$b = b - \Delta b=4-0.095=3.905$</li>
<li>$x=2w+3b=2 \times 2.667+3 \times 3.905=17.049$</li>
<li>$y=2b+1=2 \times 3.905+1=8.81$</li>
<li>$z=x \times y=17.049 \times 8.81=150.2$</li>
</ul>
<blockquote>
<p>注意：</p>
<ol>
<li>在检查 $\Delta z$ 时的值时，注意要用绝对值，因为有可能是个负数</li>
<li>在计算 $\Delta b$ 和 $\Delta w$ 时，第一次时，它们对 $z$ 的贡献值分别是 $1/63$ 和 $1/18$，但是第二次时，由于 $b,w$ 值的变化，对 $z$ 的贡献值也会有微小变化，所以要重新计算。具体解释如下：</li>
</ol>
<p>$$<br>\frac{\partial{z}}{\partial{b}}=\frac{\partial{z}}{\partial{x}} \cdot \frac{\partial{x}}{\partial{b}}+\frac{\partial{z}}{\partial{y}}\cdot\frac{\partial{y}}{\partial{b}}=y \cdot 3+x \cdot 2=3y+2x<br>$$</p>
<p>$$<br>\frac{\partial{z}}{\partial{w}}=\frac{\partial{z}}{\partial{x}} \cdot \frac{\partial{x}}{\partial{w}}+\frac{\partial{z}}{\partial{y}}\cdot\frac{\partial{y}}{\partial{w}}=y \cdot 2+x \cdot 0 = 2y<br>$$</p>
<p>所以，在每次迭代中，要重新计算下面两个值：<br>$$<br>\Delta b=\frac{\Delta z}{3y+2x}<br>$$</p>
<p>$$<br>\Delta w=\frac{\Delta z}{2y}<br>$$</p>
</blockquote>
<p>python代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目标函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">target_function</span>(<span class="params">w,b</span>):</span><br><span class="line">    x = <span class="number">2</span>*w+<span class="number">3</span>*b</span><br><span class="line">    y=<span class="number">2</span>*b+<span class="number">1</span></span><br><span class="line">    z=x*y</span><br><span class="line">    <span class="keyword">return</span> x,y,z</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单变量</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">single_variable</span>(<span class="params">w,b,t</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\nsingle variable: b ----- &quot;</span>)</span><br><span class="line">    error = <span class="number">1e-5</span></span><br><span class="line">    <span class="keyword">while</span>(<span class="literal">True</span>):</span><br><span class="line">        x,y,z = target_function(w,b)</span><br><span class="line">        delta_z = z - t</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;w=%f,b=%f,z=%f,delta_z=%f&quot;</span>%(w,b,z,delta_z))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(delta_z) &lt; error:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        factor_b = <span class="number">2</span>*x+<span class="number">3</span>*y</span><br><span class="line">        delta_b = delta_z/factor_b</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;factor_b=%f, delta_b=%f&quot;</span>%(factor_b, delta_b))</span><br><span class="line">        b = b - delta_b</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;done!&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;final b=%f&quot;</span>%b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 双变量</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">double_variable</span>(<span class="params">w,b,t</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\ndouble variable: w, b -----&quot;</span>)</span><br><span class="line">    error = <span class="number">1e-5</span></span><br><span class="line">    <span class="keyword">while</span>(<span class="literal">True</span>):</span><br><span class="line">        x,y,z = target_function(w,b)</span><br><span class="line">        delta_z = z - t</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;w=%f,b=%f,z=%f,delta_z=%f&quot;</span>%(w,b,z,delta_z))</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(delta_z) &lt; error:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        factor_b, factor_w = calculate_wb_factor(x,y)</span><br><span class="line">        delta_b = delta_z/factor_b/<span class="number">2</span></span><br><span class="line">        delta_w = delta_z/factor_w/<span class="number">2</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;factor_b=%f, factor_w=%f, delta_b=%f, delta_w=%f&quot;</span>%(factor_b, factor_w, delta_b,delta_w))</span><br><span class="line">        b = b - delta_b</span><br><span class="line">        w = w - delta_w</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;done!&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;final b=%f&quot;</span>%b)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;final w=%f&quot;</span>%w)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_wb_factor</span>(<span class="params">x,y</span>):</span><br><span class="line">    factor_b = <span class="number">2</span>*x+<span class="number">3</span>*y</span><br><span class="line">    factor_w = <span class="number">2</span>*y</span><br><span class="line">    <span class="keyword">return</span> factor_b, factor_w</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    w = <span class="number">3</span></span><br><span class="line">    b = <span class="number">4</span></span><br><span class="line">    t = <span class="number">150</span></span><br><span class="line">    single_variable(w,b,t)</span><br><span class="line">    double_variable(w,b,t)</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/run1.5riodmivbsw0.webp" alt="run1" style="zoom:80%;" />

<h3 id="非线性反向传播"><a href="#非线性反向传播" class="headerlink" title="非线性反向传播"></a>非线性反向传播</h3><p>例：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/game.5vj0wxjtcss0.webp" alt="game"></p>
<center>图game 非线性的反向传播</center>

<p>其中$1&lt;x&lt;=10,0&lt;y&lt;2.15$。假设有5个人分别代表 $x,a,b,c,y$：</p>
<h4 id="正向过程"><a href="#正向过程" class="headerlink" title="正向过程"></a>正向过程</h4><ol>
<li>第1个人，输入层，随机输入第一个 $x$ 值，$x$ 的取值范围 $(1,10]$，假设第一个数是 $2$；</li>
<li>第2个人，第一层网络计算，接收第1个人传入 $x$ 的值，计算：$a=x^2$；</li>
<li>第3个人，第二层网络计算，接收第2个人传入 $a$ 的值，计算：$b=\ln (a)$；</li>
<li>第4个人，第三层网络计算，接收第3个人传入 $b$ 的值，计算：$c=\sqrt{b}$；</li>
<li>第5个人，输出层，接收第4个人传入 $c$ 的值</li>
</ol>
<h4 id="反向过程"><a href="#反向过程" class="headerlink" title="反向过程"></a>反向过程</h4><ol start="6">
<li>第5个人，计算 $y$ 与 $c$ 的差值：$\Delta c = c - y$，传回给第4个人</li>
<li>第4个人，接收第5个人传回$\Delta c$，计算 $\Delta b = \Delta c \cdot 2\sqrt{b}$</li>
<li>第3个人，接收第4个人传回$\Delta b$，计算 $\Delta a = \Delta b \cdot a$</li>
<li>第2个人，接收第3个人传回$\Delta a$，计算 $\Delta x = \frac{\Delta}{2x}$</li>
<li>第1个人，接收第2个人传回$\Delta x$，更新 $x \leftarrow x - \Delta x$，回到第1步</li>
</ol>
<blockquote>
<p>$$<br>\frac{da}{dx}=\frac{d(x^2)}{dx}=2x=\frac{\Delta a}{\Delta x} \tag{1}<br>$$</p>
<p>$$<br>\frac{db}{da} =\frac{d(\ln{a})}{da} =\frac{1}{a} = \frac{\Delta b}{\Delta a} \tag{2}<br>$$</p>
<p>$$<br>\frac{dc}{db}=\frac{d(\sqrt{b})}{db}=\frac{1}{2\sqrt{b}}=\frac{\Delta c}{\Delta b} \tag{3}<br>$$</p>
<p>因此得到如下一组公式，可以把最后一层 $\Delta c$ 的误差一直反向传播给最前面的 $\Delta x$，从而更新 $x$ 值：<br>$$<br>\Delta c = c - y \tag{4}<br>$$</p>
<p>$$<br>\Delta b = \Delta c \cdot 2\sqrt{b}  \tag{根据式3}<br>$$</p>
<p>$$<br>\Delta a = \Delta b \cdot a  \tag{根据式2}<br>$$</p>
<p>$$<br>\Delta x = \Delta a / 2x \tag{根据式1}<br>$$</p>
</blockquote>
<p>提出问题：假设我们想最后得到 $c=2.13$ 的值，$x$ 应该是多少？（误差小于 $0.001$ 即可）</p>
<p>我们给定初始值 $x=2$，$\Delta x=0$，依次计算结果如表2-2。</p>
<center>表2-2 正向与反向的迭代计算</center>

<table>
<thead>
<tr>
<th>方向</th>
<th>公式</th>
<th>迭代1</th>
<th>迭代2</th>
<th>迭代3</th>
<th>迭代4</th>
<th>迭代5</th>
</tr>
</thead>
<tbody><tr>
<td>正向</td>
<td>$x=x-\Delta x$</td>
<td>2</td>
<td>4.243</td>
<td>7.344</td>
<td>9.295</td>
<td>9.665</td>
</tr>
<tr>
<td>正向</td>
<td>$a=x^2$</td>
<td>4</td>
<td>18.005</td>
<td>53.934</td>
<td>86.404</td>
<td>93.233</td>
</tr>
<tr>
<td>正向</td>
<td>$b=\ln(a)$</td>
<td>1.386</td>
<td>2.891</td>
<td>3.988</td>
<td>4.459</td>
<td>4.535</td>
</tr>
<tr>
<td>正向</td>
<td>$c=\sqrt{b}$</td>
<td>1.177</td>
<td>1.700</td>
<td>1.997</td>
<td>2.112</td>
<td>2.129</td>
</tr>
<tr>
<td></td>
<td>标签值y</td>
<td>2.13</td>
<td>2.13</td>
<td>2.13</td>
<td>2.13</td>
<td>2.13</td>
</tr>
<tr>
<td>反向</td>
<td>$\Delta c = c - y$</td>
<td>-0.953</td>
<td>-0.430</td>
<td>-0.133</td>
<td>-0.018</td>
<td></td>
</tr>
<tr>
<td>反向</td>
<td>$\Delta b = \Delta c \cdot 2\sqrt{b}$</td>
<td>-2.243</td>
<td>-1.462</td>
<td>-0.531</td>
<td>-0.078</td>
<td></td>
</tr>
<tr>
<td>反向</td>
<td>$\Delta a = \Delta b \cdot a$</td>
<td>-8.973</td>
<td>-26.317</td>
<td>-28.662</td>
<td>-6.698</td>
<td></td>
</tr>
<tr>
<td>反向</td>
<td>$\Delta x = \Delta a / 2x$</td>
<td>-2.243</td>
<td>-3.101</td>
<td>-1.951</td>
<td>-0.360</td>
<td></td>
</tr>
</tbody></table>
<p>python代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_fun</span>(<span class="params">X,Y</span>):</span><br><span class="line">    x = np.linspace(<span class="number">1.2</span>,<span class="number">10</span>)</span><br><span class="line">    a = x*x</span><br><span class="line">    b = np.log(a)</span><br><span class="line">    c = np.sqrt(b)</span><br><span class="line">    plt.plot(x,c)</span><br><span class="line"></span><br><span class="line">    plt.plot(X,Y,<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    d = <span class="number">1</span>/(x*np.sqrt(np.log(x**<span class="number">2</span>)))</span><br><span class="line">    plt.plot(x,d)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    a = x*x</span><br><span class="line">    b = np.log(a)</span><br><span class="line">    c = np.sqrt(b)</span><br><span class="line">    <span class="keyword">return</span> a,b,c</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">x,a,b,c,y</span>):</span><br><span class="line">    loss = c - y</span><br><span class="line">    delta_c = loss</span><br><span class="line">    delta_b = delta_c * <span class="number">2</span> * np.sqrt(b)</span><br><span class="line">    delta_a = delta_b * a</span><br><span class="line">    delta_x = delta_a / <span class="number">2</span> / x</span><br><span class="line">    <span class="keyword">return</span> loss, delta_x, delta_a, delta_b, delta_c</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">x, delta_x</span>):</span><br><span class="line">    x = x - delta_x</span><br><span class="line">    <span class="keyword">if</span> x &lt; <span class="number">1</span>:</span><br><span class="line">        x = <span class="number">1.1</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;how to play: 1) input x, 2) calculate c, 3) input target number but not faraway from c&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;input x as initial number(1.2,10), you can try 1.3:&quot;</span>)</span><br><span class="line">    line = <span class="built_in">input</span>()</span><br><span class="line">    x = <span class="built_in">float</span>(line)</span><br><span class="line"></span><br><span class="line">    a,b,c = forward(x)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;c=%f&quot;</span> %c)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;input y as target number(0.5,2), you can try 1.8:&quot;</span>)</span><br><span class="line">    line = <span class="built_in">input</span>()</span><br><span class="line">    y = <span class="built_in">float</span>(line)</span><br><span class="line"></span><br><span class="line">    error = <span class="number">1e-3</span></span><br><span class="line"></span><br><span class="line">    X,Y = [],[]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;forward...&quot;</span>)</span><br><span class="line">        a,b,c = forward(x)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;x=%f,a=%f,b=%f,c=%f&quot;</span> %(x,a,b,c))</span><br><span class="line">        X.append(x)</span><br><span class="line">        Y.append(c)</span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;backward...&quot;</span>)</span><br><span class="line">        loss, delta_x, delta_a, delta_b, delta_c = backward(x,a,b,c,y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(loss) &lt; error:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;done!&quot;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># update x</span></span><br><span class="line">        x = update(x, delta_x)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;delta_c=%f, delta_b=%f, delta_a=%f, delta_x=%f\n&quot;</span> %(delta_c, delta_b, delta_a, delta_x))</span><br><span class="line"></span><br><span class="line">    draw_fun(X,Y)</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/run2.7brhoacrw880.webp" alt="run2" style="zoom:80%;" />

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/Figure_1.74zat3mwxio0.webp" alt="Figure_1"></p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><blockquote>
<p>梯度下降（英语：Gradient descent）是一个一阶最优化算法，通常也称为最陡下降法，但是不该与近似积分的最陡下降法（英语：Method of steepest descent）混淆。 要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。</p>
</blockquote>
<h3 id="梯度下降的数学理解"><a href="#梯度下降的数学理解" class="headerlink" title="梯度下降的数学理解"></a>梯度下降的数学理解</h3><p>梯度下降的数学公式：</p>
<p>$$\theta_{n+1} = \theta_{n} - \eta \cdot \nabla J(\theta) \tag{1}$$</p>
<p>其中：</p>
<ul>
<li>$\theta_{n+1}$：下一个值；</li>
<li>$\theta_n$：当前值；</li>
<li>$-$：减号，梯度的反向；</li>
<li>$\eta$：学习率或步长，控制每一步走的距离，不要太快以免错过了最佳景点，不要太慢以免时间太长；</li>
<li>$\nabla$：梯度，函数当前位置的最快上升点；</li>
<li>$J(\theta)$：函数。</li>
</ul>
<blockquote>
<p>梯度下降的三要素</p>
<ol>
<li>当前点；</li>
<li>方向；</li>
<li>步长。</li>
</ol>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/gd_concept.lgd8sv0h3do.webp" alt="gd_concept"></p>
<center>图gd_concept 梯度下降的步骤</center>

<p>图gd_concept 解释了在函数极值点的两侧做梯度下降的计算过程，梯度下降的目的就是使得x值向极值点逼近。</p>
<h3 id="单变量函数的梯度下降"><a href="#单变量函数的梯度下降" class="headerlink" title="单变量函数的梯度下降"></a>单变量函数的梯度下降</h3><p>假设一个单变量函数：</p>
<p>$$J(x) = x ^2$$</p>
<p>我们的目的是找到该函数的最小值，于是计算其微分：</p>
<p>$$J’(x) = 2x$$</p>
<p>假设初始位置为：</p>
<p>$$x_0=1.2$$</p>
<p>假设学习率：</p>
<p>$$\eta = 0.3$$</p>
<p>根据公式(1)，迭代公式：</p>
<p>$$x_{n+1} = x_{n} - \eta \cdot \nabla J(x)= x_{n} - \eta \cdot 2x$$</p>
<p>假设终止条件为 $J(x)&lt;0.01$</p>
<p>python代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目标函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">target_function</span>(<span class="params">x</span>):</span><br><span class="line">    y = x*x</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导数函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">derivative_function</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘图函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_function</span>():</span><br><span class="line">    x = np.linspace(-<span class="number">1.2</span>,<span class="number">1.2</span>)</span><br><span class="line">    y = target_function(x)</span><br><span class="line">    plt.plot(x,y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_gd</span>(<span class="params">X</span>):</span><br><span class="line">    Y = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">        Y.append(target_function(X[i]))</span><br><span class="line"></span><br><span class="line">    plt.plot(X,Y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = <span class="number">1.2</span></span><br><span class="line">    eta = <span class="number">0.3</span></span><br><span class="line">    error = <span class="number">1e-3</span></span><br><span class="line">    X = []</span><br><span class="line">    X.append(x)</span><br><span class="line">    y = target_function(x)</span><br><span class="line">    <span class="keyword">while</span> y &gt; error:</span><br><span class="line">        x = x - eta * derivative_function(x)</span><br><span class="line">        X.append(x)</span><br><span class="line">        y = target_function(x)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;x=%f, y=%f&quot;</span> %(x,y))</span><br><span class="line"></span><br><span class="line">    draw_function()</span><br><span class="line">    draw_gd(X)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/run3.4iflh3xu2by0.webp" alt="run3"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/gd_single_variable.4aeoivvodtu.webp" alt="gd_single_variable"></p>
<h3 id="双变量的梯度下降"><a href="#双变量的梯度下降" class="headerlink" title="双变量的梯度下降"></a>双变量的梯度下降</h3><p>假设一个双变量函数：</p>
<p>$$J(x,y) = x^2 + \sin^2(y)$$</p>
<p>我们的目的是找到该函数的最小值，于是计算其微分：</p>
<p>$${\partial{J(x,y)} \over \partial{x}} = 2x$$<br>$${\partial{J(x,y)} \over \partial{y}} = 2 \sin y \cos y$$</p>
<p>假设初始位置为：</p>
<p>$$(x_0,y_0)=(3,1)$$</p>
<p>假设学习率：</p>
<p>$$\eta = 0.1$$</p>
<p>根据公式(1)，迭代过程是的计算公式：<br>$$(x_{n+1},y_{n+1}) = (x_n,y_n) - \eta \cdot \nabla J(x,y)$$<br>$$ = (x_n,y_n) - \eta \cdot (2x,2 \cdot \sin y \cdot \cos y) \tag{1}$$</p>
<p>根据公式(1)，假设终止条件为 $J(x,y)&lt;0.01$，迭代过程如表2-3所示。</p>
<center>表2-3 双变量梯度下降的迭代过程</center>

<table>
<thead>
<tr>
<th>迭代次数</th>
<th>x</th>
<th>y</th>
<th>J(x,y)</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>3</td>
<td>1</td>
<td>9.708073</td>
</tr>
<tr>
<td>2</td>
<td>2.4</td>
<td>0.909070</td>
<td>6.382415</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>15</td>
<td>0.105553</td>
<td>0.063481</td>
<td>0.015166</td>
</tr>
<tr>
<td>16</td>
<td>0.084442</td>
<td>0.050819</td>
<td>0.009711</td>
</tr>
</tbody></table>
<p>迭代16次后，$J(x,y)$ 的值为 $0.009711$，满足小于 $0.01$ 的条件，停止迭代。</p>
<p>python代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">target_function</span>(<span class="params">x,y</span>):</span><br><span class="line">    J = x**<span class="number">2</span> + np.sin(y)**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> J</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">derivative_function</span>(<span class="params">theta</span>):</span><br><span class="line">    x = theta[<span class="number">0</span>]</span><br><span class="line">    y = theta[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> np.array([<span class="number">2</span>*x,<span class="number">2</span>*np.sin(y)*np.cos(y)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_3d_surface</span>(<span class="params">x, y, z</span>):</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = Axes3D(fig)</span><br><span class="line"></span><br><span class="line">    u = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>)</span><br><span class="line">    v = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>)</span><br><span class="line">    X, Y = np.meshgrid(u, v)</span><br><span class="line">    R = np.zeros((<span class="built_in">len</span>(u), <span class="built_in">len</span>(v)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(u)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(v)):</span><br><span class="line">            R[i, j] = X[i, j]**<span class="number">2</span> + np.sin(Y[i, j])**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">    ax.plot_surface(X, Y, R, cmap=<span class="string">&#x27;rainbow&#x27;</span>)</span><br><span class="line">    plt.plot(x,y,z,c=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    theta = np.array([<span class="number">3</span>,<span class="number">1</span>])</span><br><span class="line">    eta = <span class="number">0.1</span></span><br><span class="line">    error = <span class="number">1e-2</span></span><br><span class="line"></span><br><span class="line">    X = []</span><br><span class="line">    Y = []</span><br><span class="line">    Z = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="built_in">print</span>(theta)</span><br><span class="line">        x=theta[<span class="number">0</span>]</span><br><span class="line">        y=theta[<span class="number">1</span>]</span><br><span class="line">        z=target_function(x,y)</span><br><span class="line">        X.append(x)</span><br><span class="line">        Y.append(y)</span><br><span class="line">        Z.append(z)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%d: x=%f, y=%f, z=%f&quot;</span> %(i,x,y,z))</span><br><span class="line">        d_theta = derivative_function(theta)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;    &quot;</span>,d_theta)</span><br><span class="line">        theta = theta - eta * d_theta</span><br><span class="line">        <span class="keyword">if</span> z &lt; error:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    show_3d_surface(X,Y,Z)</span><br></pre></td></tr></table></figure>

<p>运行结果三维曲面：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/GDDoubleVariable.2qk5zyzk0600.webp" alt="GDDoubleVariable"></p>
<h3 id="学习率η的选择"><a href="#学习率η的选择" class="headerlink" title="学习率η的选择"></a>学习率η的选择</h3><p>在公式表达时，学习率被表示为$\eta$。在代码里，我们把学习率定义为<code>learning_rate</code>，或者<code>eta</code>。针对上面的例子，试验不同的学习率对迭代情况的影响，如表2-5所示。</p>
<center>表2-5 不同学习率对迭代情况的影响</center>

<table>
<thead>
<tr>
<th>学习率</th>
<th>迭代路线图</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>1.0</td>
<td><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/gd100.dezcx93tuuw.webp" alt="gd100"></td>
<td>学习率太大，迭代的情况很糟糕，在一条水平线上跳来跳去，永远也不能下降。</td>
</tr>
<tr>
<td>0.8</td>
<td><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/gd080.2sw5mqw1y4q0.webp" alt="gd080"></td>
<td>学习率大，会有这种左右跳跃的情况发生，这不利于神经网络的训练。</td>
</tr>
<tr>
<td>0.4</td>
<td><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/gd040.56jcyb0oxw00.webp" alt="gd040"></td>
<td>学习率合适，损失值会从单侧下降，4步以后基本接近了理想值。</td>
</tr>
<tr>
<td>0.1</td>
<td><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/gd010.7lcpeo05lxk0.webp" alt="gd010"></td>
<td>学习率较小，损失值会从单侧下降，但下降速度非常慢，10步了还没有到达理想状态。</td>
</tr>
</tbody></table>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><blockquote>
<p>在最优化，统计学，计量经济学，决策论，机器学习和计算神经科学的领域中，损失函数或成本函数是指一种将一个事件（在一个样本空间中的一个元素）映射到一个表达与其事件相关的经济成本或机会成本的实数上的一种函数，借此直观表示的一些”成本”与事件的关联。一个最佳化问题的目标是将损失函数最小化。一个目标函数通常为一个损失函数的本身或者为其负值。当一个目标函数为损失函数的负值时，目标函数的值寻求最大化。</p>
</blockquote>
<p>“损失”就是所有样本的“误差”的总和，亦即（$m$ 为样本数）：</p>
<p>$$损失 = \sum^m_{i=1}误差_i$$</p>
<p>$$J = \sum_{i=1}^m loss_i$$</p>
<h3 id="机器学习常用损失函数"><a href="#机器学习常用损失函数" class="headerlink" title="机器学习常用损失函数"></a>机器学习常用损失函数</h3><p>符号规则：$a$ 是预测值，$y$ 是样本标签值，$loss$ 是损失函数值。</p>
<ul>
<li>Gold Standard Loss，又称0-1误差</li>
</ul>
<p>$$<br>loss=\begin{cases}<br>0 &amp; a=y \\<br>1 &amp; a \ne y<br>\end{cases}<br>$$</p>
<ul>
<li>绝对值损失函数</li>
</ul>
<p>$$<br>loss = |y-a|<br>$$</p>
<ul>
<li>Hinge Loss，铰链/折页损失函数或最大边界损失函数，主要用于SVM（支持向量机）中</li>
</ul>
<p>$$<br>loss=\max(0,1-y \cdot a) \qquad y=\pm 1<br>$$</p>
<ul>
<li>Log Loss，对数损失函数，又叫交叉熵损失函数(cross entropy error)</li>
</ul>
<p>$$<br>loss = -[y \cdot \ln (a) + (1-y) \cdot \ln (1-a)]  \qquad y \in \{ 0,1 \}<br>$$</p>
<ul>
<li>Squared Loss，均方差损失函数</li>
</ul>
<p>$$<br>loss=(a-y)^2<br>$$</p>
<ul>
<li>Exponential Loss，指数损失函数</li>
</ul>
<p>$$<br>loss = e^{-(y \cdot a)}<br>$$</p>
<h3 id="损失函数图像理解"><a href="#损失函数图像理解" class="headerlink" title="损失函数图像理解"></a>损失函数图像理解</h3><h4 id="用二维函数图像理解单变量对损失函数的影响"><a href="#用二维函数图像理解单变量对损失函数的影响" class="headerlink" title="用二维函数图像理解单变量对损失函数的影响"></a>用二维函数图像理解单变量对损失函数的影响</h4><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/gd2d.wd52hvnadhc.webp" alt="gd2d"></p>
<center>图gd2d 单变量的损失函数图</center>

<p>图gd2d中，纵坐标是损失函数值，横坐标是变量。不断地改变变量的值，会造成损失函数值的上升或下降。而梯度下降算法会让我们沿着损失函数值下降的方向前进。</p>
<ol>
<li>假设我们的初始位置在 $A$ 点，$x=x_0$，损失函数值（纵坐标）较大，回传给网络做训练；</li>
<li>经过一次迭代后，我们移动到了 $B$ 点，$x=x_1$，损失函数值也相应减小，再次回传重新训练；</li>
<li>以此节奏不断向损失函数的最低点靠近，经历了 $x_2,x_3,x_4,x_5$；</li>
<li>直到损失值达到可接受的程度，比如 $x_5$ 的位置，就停止训练。</li>
</ol>
<h4 id="用等高线图理解双变量对损失函数影响"><a href="#用等高线图理解双变量对损失函数影响" class="headerlink" title="用等高线图理解双变量对损失函数影响"></a>用等高线图理解双变量对损失函数影响</h4><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/gd3d.5xlrcfexi080.webp" alt="gd3d"></p>
<center>图gd3d 双变量的损失函数图</center>

<p>图gd3d中，横坐标是一个变量 $w$，纵坐标是另一个变量 $b$。两个变量的组合形成的损失函数值，在图中对应处于等高线上的唯一的一个坐标点。$w,b$ 所有不同值的组合会形成一个损失函数值的矩阵，我们把矩阵中具有相同（相近）损失函数值的点连接起来，可以形成一个不规则椭圆，其圆心位置，是损失值为 $0$ 的位置，也是我们要逼近的目标。</p>
<p>这个椭圆如同平面地图的等高线，来表示的一个洼地，中心位置比边缘位置要低，通过对损失函数值的计算，对损失函数的求导，会带领我们沿着等高线形成的梯子一步步下降，无限逼近中心点。</p>
<h3 id="神经网络中常用的损失函数"><a href="#神经网络中常用的损失函数" class="headerlink" title="神经网络中常用的损失函数"></a>神经网络中常用的损失函数</h3><ul>
<li><p>均方差函数，主要用于回归（Regression）</p>
</li>
<li><p>交叉熵函数，主要用于分类（Classification）</p>
</li>
</ul>
<p>二者都是非负函数，极值在底部，用梯度下降法可以求解。</p>
<h2 id="学习心得"><a href="#学习心得" class="headerlink" title="学习心得"></a>学习心得</h2><blockquote>
<p>通过本章结我了解到神经网络（Neural Networks）的基础知识。人工智能的学者通过构建神经元模型来进行模拟真正的神经元，而将多个独立的神经元叠加起来，就获得了一个类神经网络，这么一个神经网络拥有的一部分神奇的地方在于，你只需要输入x，就能得到输出y，所有的中间训练过程机器都会自己完成。</p>
</blockquote>
<p>如果你想要更多的学习可以看<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV164411m79z?p=2">大佬的讲解</a></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://github.com/Microsoft/ai-edu">GitHub - microsoft/ai-edu: AI education materials for Chinese students, teachers and IT professionals.</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://sonna.cn">yanqi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://sonna.cn/2021/06/07/NNs/">https://sonna.cn/2021/06/07/NNs/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://sonna.cn" target="_blank">雪天困不住的大孩子</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/06/10/DML/" title="DML"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/082.391vqs7d7h40.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">DML</div></div></a></div><div class="next-post pull-right"><a href="/2021/05/28/DDL/" title="DDL"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/20220423/077.3vqfz49gxc60.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">DDL</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx@1.0.11/img/me.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">yanqi</div><div class="author-info__description">梦想去做一名全栈工程师</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/yanqi17118357"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">人活在世上不就是为了樱岛麻衣吗？</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E6%95%B0%E5%AD%A6%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">神经元的数学模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%85%A5-input"><span class="toc-number">1.1.</span> <span class="toc-text">输入 input</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%83%E9%87%8D-weights"><span class="toc-number">1.2.</span> <span class="toc-text">权重 weights</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%8F%E7%A7%BB-bias"><span class="toc-number">1.3.</span> <span class="toc-text">偏移 bias</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%82%E5%92%8C%E8%AE%A1%E7%AE%97-sum"><span class="toc-number">1.4.</span> <span class="toc-text">求和计算 sum</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-activation"><span class="toc-number">1.5.</span> <span class="toc-text">激活函数 activation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">训练流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.</span> <span class="toc-text">反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%90%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%AE%9E%E4%BE%8B"><span class="toc-number">3.1.</span> <span class="toc-text">正向计算的实例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B1%82%E8%A7%A3-w"><span class="toc-number">3.2.</span> <span class="toc-text">反向传播求解 $w$</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B1%82-w-%E7%9A%84%E5%81%8F%E5%AF%BC"><span class="toc-number">3.2.1.</span> <span class="toc-text">求 $w$ 的偏导</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B1%82-w-%E7%9A%84%E5%85%B7%E4%BD%93%E5%8F%98%E5%8C%96%E5%80%BC"><span class="toc-number">3.2.2.</span> <span class="toc-text">求 $w$ 的具体变化值</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B1%82%E8%A7%A3-b"><span class="toc-number">3.3.</span> <span class="toc-text">反向传播求解 $b$</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B1%82-b-%E7%9A%84%E5%81%8F%E5%AF%BC"><span class="toc-number">3.3.1.</span> <span class="toc-text">求 $b$ 的偏导</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B1%82-b-%E7%9A%84%E5%85%B7%E4%BD%93%E5%8F%98%E5%8C%96%E5%80%BC"><span class="toc-number">3.3.2.</span> <span class="toc-text">求 $b$ 的具体变化值</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8C%E6%97%B6%E6%B1%82%E8%A7%A3-w-%E5%92%8C-b-%E7%9A%84%E5%8F%98%E5%8C%96%E5%80%BC"><span class="toc-number">3.4.</span> <span class="toc-text">同时求解 $w$ 和 $b$ 的变化值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.5.</span> <span class="toc-text">非线性反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%90%91%E8%BF%87%E7%A8%8B"><span class="toc-number">3.5.1.</span> <span class="toc-text">正向过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E8%BF%87%E7%A8%8B"><span class="toc-number">3.5.2.</span> <span class="toc-text">反向过程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">4.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%95%B0%E5%AD%A6%E7%90%86%E8%A7%A3"><span class="toc-number">4.1.</span> <span class="toc-text">梯度下降的数学理解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E5%8F%98%E9%87%8F%E5%87%BD%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">4.2.</span> <span class="toc-text">单变量函数的梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8C%E5%8F%98%E9%87%8F%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">4.3.</span> <span class="toc-text">双变量的梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%CE%B7%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">4.4.</span> <span class="toc-text">学习率η的选择</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.1.</span> <span class="toc-text">机器学习常用损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F%E7%90%86%E8%A7%A3"><span class="toc-number">5.2.</span> <span class="toc-text">损失函数图像理解</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%A8%E4%BA%8C%E7%BB%B4%E5%87%BD%E6%95%B0%E5%9B%BE%E5%83%8F%E7%90%86%E8%A7%A3%E5%8D%95%E5%8F%98%E9%87%8F%E5%AF%B9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">5.2.1.</span> <span class="toc-text">用二维函数图像理解单变量对损失函数的影响</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%A8%E7%AD%89%E9%AB%98%E7%BA%BF%E5%9B%BE%E7%90%86%E8%A7%A3%E5%8F%8C%E5%8F%98%E9%87%8F%E5%AF%B9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%BD%B1%E5%93%8D"><span class="toc-number">5.2.2.</span> <span class="toc-text">用等高线图理解双变量对损失函数影响</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.3.</span> <span class="toc-text">神经网络中常用的损失函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E5%BF%83%E5%BE%97"><span class="toc-number">6.</span> <span class="toc-text">学习心得</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">7.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/09/12/%E6%8C%82%E8%BD%BDmmcblk1%E7%A3%81%E7%9B%98%E5%88%86%E5%8C%BA/" title="挂载mmcblk1磁盘分区"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://s2.loli.net/2023/09/12/m4lSkpJeV18jMhy.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="挂载mmcblk1磁盘分区"/></a><div class="content"><a class="title" href="/2023/09/12/%E6%8C%82%E8%BD%BDmmcblk1%E7%A3%81%E7%9B%98%E5%88%86%E5%8C%BA/" title="挂载mmcblk1磁盘分区">挂载mmcblk1磁盘分区</a><time datetime="2023-09-12T10:57:16.000Z" title="发表于 2023-09-12 10:57:16">2023-09-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/13/Java%E5%9F%BA%E7%A1%80%E5%AE%B9%E5%99%A8/" title="Java基础容器"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/img/244.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Java基础容器"/></a><div class="content"><a class="title" href="/2022/07/13/Java%E5%9F%BA%E7%A1%80%E5%AE%B9%E5%99%A8/" title="Java基础容器">Java基础容器</a><time datetime="2022-07-13T00:46:14.000Z" title="发表于 2022-07-13 00:46:14">2022-07-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/05/Spring5%E6%A1%86%E6%9E%B6/" title="Spring5"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/img/234.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spring5"/></a><div class="content"><a class="title" href="/2022/07/05/Spring5%E6%A1%86%E6%9E%B6/" title="Spring5">Spring5</a><time datetime="2022-07-05T08:57:16.000Z" title="发表于 2022-07-05 08:57:16">2022-07-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/04/CycleGAN/" title="图片场景转换-基于CycleGAN"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/img/206.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="图片场景转换-基于CycleGAN"/></a><div class="content"><a class="title" href="/2022/07/04/CycleGAN/" title="图片场景转换-基于CycleGAN">图片场景转换-基于CycleGAN</a><time datetime="2022-07-04T08:57:16.000Z" title="发表于 2022-07-04 08:57:16">2022-07-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/20/%E7%A7%8D%E6%A0%91%E7%9A%84%E7%94%B7%E4%BA%BA/" title="种树的男人"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://npm.elemecdn.com/yanqi1711-picx/pic/1.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="种树的男人"/></a><div class="content"><a class="title" href="/2022/06/20/%E7%A7%8D%E6%A0%91%E7%9A%84%E7%94%B7%E4%BA%BA/" title="种树的男人">种树的男人</a><time datetime="2022-06-20T08:57:16.000Z" title="发表于 2022-06-20 08:57:16">2022-06-20</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 By yanqi</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://npm.elemecdn.com/@fancyapps/ui/dist/fancybox/fancybox.umd.js"></script><script src="https://npm.elemecdn.com/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://npm.elemecdn.com/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://npm.elemecdn.com/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (true){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><div class="aplayer no-destroy" data-id="7426255953" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="false" muted></div><link rel="stylesheet" href="https://npm.elemecdn.com/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://npm.elemecdn.com/aplayer/dist/APlayer.min.js"></script><script src="https://npm.elemecdn.com/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://npm.elemecdn.com/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://npm.elemecdn.com/algoliasearch/dist/algoliasearch-lite.umd.js"></script><script src="https://npm.elemecdn.com/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></div></body></html>